<div align="center">

# ğŸ§  EdgePresence

### Real-Time Emotion, Voice & Engagement Analysis â€” Entirely in the Browser

[![TensorFlow.js](https://img.shields.io/badge/TensorFlow.js-4.22-FF6F00?logo=tensorflow&logoColor=white)](https://www.tensorflow.org/js)
[![Astro](https://img.shields.io/badge/Astro-5.17-BC52EE?logo=astro&logoColor=white)](https://astro.build)
[![React](https://img.shields.io/badge/React-19-61DAFB?logo=react&logoColor=black)](https://react.dev)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.9-3178C6?logo=typescript&logoColor=white)](https://www.typescriptlang.org)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

**No backend. No API calls. No data leaves your device.**  
Neural networkâ€“powered face detection, emotion classification, voice analysis, and engagement scoring â€” running 100% client-side via WebGL-accelerated TensorFlow.js.

---

[Features](#-features) Â· [Architecture](#-architecture) Â· [Quick Start](#-quick-start) Â· [How It Works](#-how-it-works) Â· [Model Training](#-model-training) Â· [Project Structure](#-project-structure) Â· [Tech Stack](#-tech-stack)

</div>

---

## âœ¨ Features

| Capability | How It Works | Runs On |
|:---|:---|:---|
| ğŸ­ **Face Detection** | MediaPipe FaceMesh (468 landmarks) via TensorFlow.js | WebGL 2 |
| ğŸ˜Š **Emotion Classification** | Custom CNN trained on FER-2013 (7 emotions) | WebGL 2 |
| ğŸ™ï¸ **Voice Analysis** | Web Audio API â€” RMS energy + speech detection | AudioContext |
| ğŸ“Š **Engagement Scoring** | Weighted fusion: emotion 40% + voice 30% + speech 30% | CPU |
| ğŸ”’ **100% Client-Side** | Zero backend, zero network calls â€” your data never leaves the browser | â€” |
| âš¡ **Real-Time** | `requestAnimationFrame` loop with temporal smoothing for stable output | â€” |
| ğŸ“ˆ **Live Performance Metrics** | FPS, inference latency, tensor count, memory usage | â€” |

### Detected Emotions

> **Angry** Â· **Disgust** Â· **Fear** Â· **Happy** Â· **Sad** Â· **Surprise** Â· **Neutral**

---

## ğŸ›ï¸ Architecture

```mermaid
flowchart LR
  %% Everything runs in the browser (no backend).

  subgraph B[Browser / Client-Side Only]
    subgraph V[Video Pipeline]
      Cam["Camera<br/>(getUserMedia)"] --> FD["Face Detector<br/>(MediaPipe FaceMesh via TFJS)"]
      FD --> Crop["Face Crop + Preprocess<br/>48Ã—48 grayscale, normalize 0..1"]
      Crop --> EC["Emotion Classifier<br/>(Custom CNN via TFJS WebGL)"]
      EC --> TS["Temporal Smoother<br/>(EMA)"]
    end

    subgraph A[Audio Pipeline]
      Mic["Microphone<br/>(getUserMedia)"] --> AE["Audio Engine<br/>(Web Audio API: RMS + speech detect)"]
      AE --> EE["Engagement Engine<br/>(weighted scoring)"]
    end

    PM["Performance Monitor<br/>(FPS, latency, tensors, memory)"]
    UI["React UI<br/>(Live dashboards)"]

    TS --> UI
    EE --> UI
    PM --> UI
  end

  %% Rendering hints
  classDef core fill:#0b1021,stroke:#5df2d6,color:#e9edf5;
  class FD,EC,AE,EE,TS,PM core;
```

---

## ğŸš€ Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/your-username/TensorflowJS-ReactivAI.git
cd TensorflowJS-ReactivAI

# 2. Install dependencies
npm install

# 3. Start the dev server
npm run dev
```

Open **http://localhost:4321/TensorflowJS-ReactivAI/** â€” grant camera and microphone access, and you're live.

### Commands

| Command | Action |
|:---|:---|
| `npm install` | Install dependencies |
| `npm run dev` | Start dev server at `localhost:4321` |
| `npm run build` | Production build â†’ `./dist/` |
| `npm run preview` | Preview production build locally |
| `npm run test` | Run unit tests (Vitest) |

---

## ğŸ”¬ How It Works

### 1. Face Detection â€” MediaPipe FaceMesh

The app uses **MediaPipe FaceMesh** through TensorFlow.js to detect faces and extract 468 facial landmarks in real time. Each video frame is snapshot to an off-screen canvas and passed to the model, avoiding WebGL texture conflicts with React's DOM management.

```
Camera â†’ HTMLVideoElement â†’ Off-screen Canvas snapshot â†’ FaceMesh model â†’ 468 keypoints + bounding box
```

- Bounding box computed from keypoint extremes with 10% padding
- Single-face mode for performance
- ~20â€“40ms per detection on modern GPUs

### 2. Emotion Classification â€” Custom CNN

A dedicated convolutional neural network classifies the detected face region into one of **7 emotions**. The face is extracted, resized to **48Ã—48 grayscale**, normalized to `[0, 1]`, and passed through the model.

**Model Architecture:**

```
Input (48Ã—48Ã—1)
    â”œâ”€ Conv2D(32, 3Ã—3) â†’ BatchNorm â†’ Conv2D(32, 3Ã—3) â†’ BatchNorm â†’ MaxPool â†’ Dropout(0.25)
    â”œâ”€ Conv2D(64, 3Ã—3) â†’ BatchNorm â†’ Conv2D(64, 3Ã—3) â†’ BatchNorm â†’ MaxPool â†’ Dropout(0.25)
    â”œâ”€ Conv2D(128, 3Ã—3) â†’ BatchNorm â†’ Conv2D(128, 3Ã—3) â†’ BatchNorm â†’ MaxPool â†’ Dropout(0.25)
    â”œâ”€ Flatten â†’ Dense(256) â†’ BatchNorm â†’ Dropout(0.5)
    â”œâ”€ Dense(128) â†’ BatchNorm â†’ Dropout(0.3)
    â””â”€ Dense(7, softmax) â†’ Probability distribution over 7 emotions
```

- **1.5M parameters** / ~5.7 MB in browser
- Trained on **FER-2013** (35,887 facial expression images)
- Augmented with flips, rotation, zoom, and translation
- Class-weighted loss to handle imbalanced classes (e.g., disgust has only 436 training samples)
- **Temporal smoothing** via exponential moving average eliminates jitter between frames

### 3. Voice Analysis â€” Web Audio API

Real-time audio analysis runs entirely in-browser using the **Web Audio API**:

```
Microphone â†’ MediaStream â†’ AudioContext â†’ AnalyserNode â†’ Float32 time-domain data â†’ RMS Energy
```

- **RMS Energy**: root mean square of the waveform â€” measures volume/intensity
- **Speech Detection**: threshold-based binary classification (speaking vs. silent)
- **Speech Continuity**: rolling 30-frame window tracking ratio of speaking frames
- Echo cancellation, noise suppression, and auto-gain enabled at capture

### 4. Engagement Scoring â€” Multi-Signal Fusion

The engagement engine fuses all three signals into a **0â€“100 composite score**:

$$\text{Score} = \bigl(\underbrace{E_{\text{confidence}}}_{\text{Emotion}} \times 0.4\bigr) + \bigl(\underbrace{V_{\text{energy}}}_{\text{Voice}} \times 0.3\bigr) + \bigl(\underbrace{S_{\text{continuity}}}_{\text{Speech}} \times 0.3\bigr) \times 100$$

| Component | Weight | Signal Source |
|:---|:---|:---|
| Emotion Confidence | 40% | Softmax confidence from CNN |
| Voice Energy | 30% | Normalized RMS from microphone |
| Speech Continuity | 30% | Rolling speaking ratio (30-frame window) |

### 5. Performance Monitoring

A built-in performance monitor tracks:

- **FPS** â€” frames processed per second
- **Face Detection Latency** â€” ms per face detection call
- **Emotion Inference Latency** â€” ms per CNN forward pass
- **Tensor Count & Memory** â€” active WebGL tensors and allocated bytes (leak detection)

---

## ğŸ‹ï¸ Model Training

The emotion CNN is trained offline in Python and exported to TensorFlow.js format using a **custom pure-Python converter** (no `tensorflowjs` pip package required).

### Prerequisites

```bash
pip install -r scripts/requirements-python-training.txt
# â†’ tensorflow==2.15.0, numpy==1.26.4, Pillow
```

### Train on FER-2013

```bash
# From image directory (recommended â€” Kaggle's pre-split format)
python scripts/train-emotion-model.py \
  --data-dir /path/to/fer2013 \
  --epochs 50 \
  --batch-size 64

# From CSV file
python scripts/train-emotion-model.py \
  --data /path/to/fer2013.csv \
  --epochs 50

# Quick pipeline test with synthetic data
python scripts/train-emotion-model.py --synthetic
```

The script automatically:

1. Loads and normalizes images (48Ã—48 grayscale, `[0, 1]`)
2. Computes per-class weights (capped at 3Ã—) for imbalanced classes
3. Trains with augmentation (flips, rotation, zoom, shift)
4. Saves Keras model to `artifacts/emotion_model.keras`
5. Converts to TensorFlow.js at `public/models/emotion_model/`

### Custom Keras â†’ TFJS Converter

The standard `tensorflowjs` Python package is notoriously difficult to install. We built a **pure-Python converter** that produces identical output:

```bash
python scripts/convert_keras_to_tfjs.py \
  --keras artifacts/emotion_model.keras \
  --out public/models/emotion_model
```

It handles:
- Keras 3 â†’ TFJS-compatible topology stripping (`module`, `registered_name`, `build_config`)
- Weight name sanitization (removes `:0` suffixes)
- Single-shard binary packing (`group1-shard1of1.bin`)

### A/B Testing Models

Switch models at runtime via URL parameter:

```
http://localhost:4321/TensorflowJS-ReactivAI/?model=emotion_model_py
```

---

## ğŸ“ Project Structure

```
TensorflowJS-ReactivAI/
â”œâ”€â”€ public/
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ emotion_model/          # TFJS model files served as static assets
â”‚           â”œâ”€â”€ model.json          #   Model topology + weights manifest
â”‚           â””â”€â”€ group1-shard1of1.bin #   Binary weight data (~5.7 MB)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ index.astro             # Main app page
â”‚   â”‚   â””â”€â”€ debug.astro             # Face detection debugger
â”‚   â”‚
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ EmotionAnalyzer.tsx      # Main React component (camera, loop, UI)
â”‚   â”‚
â”‚   â”œâ”€â”€ ml/                          # Machine learning modules
â”‚   â”‚   â”œâ”€â”€ tfSetup.ts              #   WebGL 2 backend initialization
â”‚   â”‚   â”œâ”€â”€ faceDetector.ts         #   MediaPipe FaceMesh wrapper
â”‚   â”‚   â”œâ”€â”€ emotionClassifier.ts    #   CNN model loading + inference
â”‚   â”‚   â””â”€â”€ temporalSmoother.ts     #   EMA filter for stable predictions
â”‚   â”‚
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â””â”€â”€ audioEngine.ts          # Web Audio API: RMS energy + speech detect
â”‚   â”‚
â”‚   â”œâ”€â”€ scoring/
â”‚   â”‚   â””â”€â”€ engagementEngine.ts     # Multi-signal engagement scoring
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â””â”€â”€ performanceMonitor.ts   # FPS, latency, memory tracking
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ math.ts                 # Clamp, lerp, mean, std deviation
â”‚   â”‚   â””â”€â”€ normalization.ts        # Pixel normalization, z-score, min-max
â”‚   â”‚
â”‚   â””â”€â”€ shims/                       # Browser shims for Node-only packages
â”‚       â”œâ”€â”€ node-fetch.ts
â”‚       â””â”€â”€ whatwg-url.ts
â”‚
â”œâ”€â”€ scripts/                         # Offline training pipeline (Python)
â”‚   â”œâ”€â”€ train-emotion-model.py      #   Full training script (FER-2013 / synthetic)
â”‚   â”œâ”€â”€ convert_keras_to_tfjs.py    #   Pure-Python Keras â†’ TFJS converter
â”‚   â”œâ”€â”€ train-emotion-model.mjs     #   Legacy JS training pipeline
â”‚   â”œâ”€â”€ test_model.py               #   Model diagnostic tests
â”‚   â”œâ”€â”€ verify_conversion.py        #   Bit-level weight verification
â”‚   â””â”€â”€ requirements-python-training.txt
â”‚
â”œâ”€â”€ types/                           # TypeScript declarations
â”œâ”€â”€ astro.config.mjs                # Astro config (static output, GitHub Pages)
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ vitest.config.ts
â””â”€â”€ package.json
```

---

## ğŸ› ï¸ Tech Stack

### Runtime (Browser)

| Technology | Role |
|:---|:---|
| **TensorFlow.js 4.22** | Neural network inference (WebGL 2 GPU-accelerated) |
| **MediaPipe FaceMesh** | 468-point facial landmark detection |
| **Web Audio API** | Real-time microphone RMS energy & speech detection |
| **WebRTC** | Camera access via `getUserMedia` |
| **React 19** | Reactive UI with real-time metric dashboards |
| **Astro 5.17** | Static site framework (zero JS overhead for shell) |
| **TypeScript 5.9** | Type safety across all modules |

### Training (Offline, Python)

| Technology | Role |
|:---|:---|
| **TensorFlow / Keras 2.15** | CNN architecture, training, and augmentation |
| **FER-2013 Dataset** | 35,887 labeled facial expression images |
| **Pillow** | Fast image I/O (grayscale conversion, resizing) |
| **Custom TFJS Converter** | Pure-Python Keras â†’ TFJS export (no `tensorflowjs` dep) |
| **NumPy 1.26** | Numerical operations and data manipulation |

### Testing

| Technology | Role |
|:---|:---|
| **Vitest 4** | Unit tests for math, normalization, scoring, smoothing |

---

## ğŸ”‘ Key Design Decisions

### Why No Backend?

Privacy and portability. All neural network inference runs on the user's GPU via WebGL. Camera frames and microphone audio are processed locally â€” nothing is transmitted over the network. The entire app deploys as static files to GitHub Pages.

### Why a Custom TFJS Converter?

The official `tensorflowjs` Python package has heavy native dependencies and frequent installation failures. Our pure-Python converter (`convert_keras_to_tfjs.py`) produces **bit-identical** output using only `tensorflow` + `numpy` â€” verified by comparing all 22 weight tensors against the original Keras model.

### Why Temporal Smoothing?

Raw per-frame emotion predictions are noisy â€” a face might flicker between "Happy" and "Neutral" across consecutive frames. An **exponential moving average** (Î± = 0.3) stabilizes output while remaining responsive to genuine expression changes.

### Why Canvas Snapshot for Face Detection?

Passing `HTMLVideoElement` directly to `estimateFaces()` fails when the video is rendered inside React's component tree â€” WebGL's `texImage2D` cannot reliably read pixels from a video whose layout is managed by CSS transforms and positioned containers. Snapshotting to an off-screen canvas solves this reliably.

---

## ğŸ“„ License

This project is licensed under the **Apache License 2.0**. See [LICENSE](LICENSE).

---

<div align="center">

**Built with neural networks, Web APIs, and zero backend dependencies.**

*All AI inference happens on your device. Your data stays yours.*

</div>
